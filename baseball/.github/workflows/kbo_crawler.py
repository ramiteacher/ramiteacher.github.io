from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime, timedelta

def crawl_kbo_last_2weeks(filename="wanle.json"):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920x1080")

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)

    games = []
    start_date = datetime(2025, 3, 22)  # ÌÅ¨Î°§ÎßÅ ÏãúÏûë ÎÇ†Ïßú
    
    for i in range(14):
        target_date = datetime.today() - timedelta(days=i)
        if target_date < start_date:  # 2025ÎÖÑ 3Ïõî 22Ïùº Ïù¥Ï†Ñ Îç∞Ïù¥ÌÑ∞Îäî Í±¥ÎÑàÎúÄ
            continue
        date_str = target_date.strftime("%Y%m%d")
        human_date = target_date.strftime("%Y-%m-%d")
        url = f"https://www.koreabaseball.com/Schedule/GameCenter/Main.aspx?gameDate={date_str}"

        print(f"üìÜ {human_date} ÌÅ¨Î°§ÎßÅ Ï§ë...")
        try:
            driver.get(url)
            time.sleep(2)
            soup = BeautifulSoup(driver.page_source, "html.parser")

            # Ïã§Ï†ú ÌéòÏù¥ÏßÄÏóêÏÑú ÌëúÏãúÎêú ÎÇ†Ïßú ÌôïÏù∏
            page_date_element = soup.select_one(".date-txt")
            if not page_date_element:
                print(f"‚ùå {human_date} ÎÇ†Ïßú Ï†ïÎ≥¥Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")
                continue

            # ÌéòÏù¥ÏßÄ ÎÇ†ÏßúÎ•º YYYY-MM-DD ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
            raw_page_date = page_date_element.text.strip()  # Ïòà: "2025.03.24(Ïõî)"
            try:
                page_date = datetime.strptime(raw_page_date.split("(")[0], "%Y.%m.%d").strftime("%Y-%m-%d")
            except ValueError:
                print(f"‚ùå {raw_page_date} ÎÇ†Ïßú ÌòïÏãùÏùÑ Î≥ÄÌôòÌï† Ïàò ÏóÜÏäµÎãàÎã§.")
                continue

            if page_date != human_date:  # URLÏùò ÎÇ†ÏßúÏôÄ Ïã§Ï†ú ÌéòÏù¥ÏßÄ ÎÇ†ÏßúÍ∞Ä Îã§Î•¥Î©¥ Í±¥ÎÑàÎúÄ
                print(f"‚è© {human_date} ‚Üí ÌéòÏù¥ÏßÄ ÎÇ†Ïßú {page_date}, Í±¥ÎÑàÎúÅÎãàÎã§.")
                continue

            rows = soup.select("li.game-cont.end")
            for li in rows:
                away_team = li.get("away_nm", "").strip()
                home_team = li.get("home_nm", "").strip()
                away_score = li.select_one(".team.away .score").text.strip()
                home_score = li.select_one(".team.home .score").text.strip()

                games.append({
                    "date": page_date,  # Ïã§Ï†ú ÌéòÏù¥ÏßÄ ÎÇ†Ïßú ÏÇ¨Ïö©
                    "home": home_team,
                    "away": away_team,
                    "score": f"{home_score} : {away_score}",
                    "highlight": ""
                })
        except Exception as e:
            print("‚ùå ÏóêÎü¨:", e)
            continue

    driver.quit()

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(games, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ Ï¥ù {len(games)}Í≤ΩÍ∏∞ Ï†ÄÏû• ÏôÑÎ£å ‚Üí {filename}")

if __name__ == "__main__":
    crawl_kbo_last_2weeks()
